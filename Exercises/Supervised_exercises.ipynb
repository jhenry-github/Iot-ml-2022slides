{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfaf54e-ae2f-473e-891e-694e210f9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, we want to explore supervised learning the way we did in class, and build models\n",
    "# we can run on a micro-controller like the ESP 32. To run this exercise, you will need Python 3.x and\n",
    "# Jupyter notebook. One way is to use Jupyter online (see at https://jupyter.org), another one is to \n",
    "# install Jupyter notebook on your computer, a third way is to install the Anaconda navigator on your computer.\n",
    "# This third way is of course better suited for those who will want to work further with ML\n",
    "# (see at https://anaconda.org/anaconda/anaconda-navigator). Choose a way and proceed with Jupyter.\n",
    "# Note: in Jupyter, 'enter' gets you another line in the same block. \"Shift enter\" executes the current block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's import some libraries we will need for our computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76806b-1dd2-4e09-afcf-64cbc15a8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let's import our training set. In your case, the path will be different, modify to match your path to unconv_MV_v5.csv\n",
    "df = pd.read_csv('/Users/jerhenry/Documents/Perso/IMT/IMT_ML_IoT/unconv_MV_v5.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bed90-8313-4332-9cc5-25e41c058e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good first step is to look at your data, we plot an 18 by 12 figure\n",
    "plt.figure(figsize=(18,12))\n",
    "# we plot with the x axis taken from the viscosity, Por, and the y axis taken from the pressure, Prod\n",
    "plt.plot(df[['Por']], df[['Prod']], 'o')\n",
    "# And we add caption and legend\n",
    "plt.title(\"Optimal Pump Pressure Measurements\")\n",
    "plt.xlabel(\"Viscosity (Por)\")\n",
    "plt.ylabel(\"Pressure (Prod)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f05bdd-d0f9-4d6c-b4b8-21769683d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another good step is to look at your data, the 'df' we created above, head gives us the first 5 lines, but you can \n",
    "# put another number, for example df.head(10) would show the first 10 entries\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380bb1b-9ff3-4eb2-98d5-3e77b2c67154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving into gradient descent. We first create a shorthand notation, x for the viscosity, y for the pressure column \n",
    "x = np.array(df[['Por']])\n",
    "y = np.array(df[['Prod']])\n",
    "# we also extract the lentgh of these columns, so we can run the loop on all 'n' entries in the columns\n",
    "n = len(df[['Por']])\n",
    "# We pick up two initial values for theta0 and theta1. You could use random values, but it is common to start with 0s\n",
    "th0_curr = th1_curr = 0\n",
    "# We also need to decide how many times wil will run the loop. It is common to start with something like 1000, then refine later\n",
    "iterations = 1000\n",
    "#then, we need to decide by how much we change theta0 and theta1, for now let's use a fixed number, something small\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e38103-e279-4f2d-889c-47a4db77c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we run our loop, for the number of iterations we decided above\n",
    "for i in range(iterations):\n",
    "    #at each step,  we take the x value, and use our theta0 and theta1 to predict some y value (likely wrong at the beginning)\n",
    "    y_predicted = th1_curr * x + th0_curr\n",
    "    #as we need to modify a bit theta0 and theta1 at each step, we calculate (at each step), the derivative of each theta\n",
    "    dth1 = -(2/n)*sum(x*(y - y_predicted))\n",
    "    dth0 = -(2/n)*sum(y - y_predicted)\n",
    "    #then our next theta is going to be changed by the value of the derivative times the learning rate. Think about what happens here:\n",
    "    # if the derivative is positive (we are going down toward 0, which is our goal, as we reach a minimum when the derivative is 0)\n",
    "    # then the next value of theta will be a bit smaller than the previous one. If the derivative is negative (we are too low),\n",
    "    # then the next value of theta will be a bit larger than the previous one (going back up toward 0).\n",
    "    # At the same time, as the derivative gets closer to 0, we change theta by a smaller and smaller value, to avoid missing the minimum\n",
    "    th1_curr = th1_curr - learning_rate * dth1\n",
    "    th0_curr = th0_curr - learning_rate * dth0\n",
    "    # one good way to see what is going on is to print at each iteration the thetas and the cost\n",
    "    # if everything works well, then the cost should be going down. So we don't need the cost for the loop itself,\n",
    "    # but we want to compute it here, just so we can print it and see if it is going down:\n",
    "    cost = (1/n) * sum ([val**2 for val in (y - y_predicted)])\n",
    "    print(\"th1 {}, th0 {}, cost {}, iteration {}\".format(th1_curr,th0_curr,cost,i))\n",
    "    # if the cost is going down too slow, use a larger learning rate. If the cost is not going down, your learning rate is too large\n",
    "    # Here, try with a learning rate of 0.1 (bounces around the minumum, too large), and 0.0001 (too slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd7176-c3c8-4699-8efd-ead7586bc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot our data again, and overlay there the line we found\n",
    "# As our x values range from 5 to 25, we just compute two points on the line, one at x=5.5 and the other at x=24.5 (we compute the predicted\n",
    "# y for each, now that we have our thetas). The below code is ugly, but the goal is to show you what happens, even if you do not master python:\n",
    "A1 = 5.5\n",
    "B1 = int(th1_curr * A1 + th0_curr)\n",
    "A2 = 24.5 \n",
    "B2 = int(th1_curr * A2 + th0_curr)\n",
    "P1 = [A1, A2]\n",
    "P2 = [B1, B2]\n",
    "# the same figure as before:\n",
    "plt.figure(figsize=(18,12))\n",
    "plt.plot(df[['Por']], df[['Prod']], 'o')\n",
    "plt.title(\"Optimal Pump Pressure Measurements\")\n",
    "plt.xlabel(\"Viscosity (Por)\")\n",
    "plt.ylabel(\"Pressure (Prod)\")\n",
    "# adding our line:\n",
    "plt.plot(P1, P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c010a-80d5-4352-9403-7e2c0b202869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's automate all this instead, by using scikit learn libraries, in particular the linear model:\n",
    "from sklearn import linear_model\n",
    "# let's create a linear regression object\n",
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec588d83-9306-4e2c-afd6-9faf7dc111a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then all the training we did above is summarized in a single command, taking as parameters the x and y columns we train against:\n",
    "reg.fit(df[['Por']],df[['Prod']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf530f25-0a3e-4b7a-bc39-c3485722ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All done. Let's plot the whole thing again:\n",
    "plt.figure(figsize=(18,12))\n",
    "plt.plot(df[['Por']], df[['Prod']], 'o')\n",
    "plt.title(\"Optimal Pump Pressure Measurements\")\n",
    "plt.xlabel(\"Viscosity (Por)\")\n",
    "plt.ylabel(\"Pressure (Prod)\")\n",
    "# adding our predicted line, this time in green (again, there is  a better way, using this heavy handed for clarity):\n",
    "A3 = 5.5\n",
    "B3 = int(reg.predict([[A3]]))\n",
    "A4 = 24.5 \n",
    "B4 = int(reg.predict([[A4]]))\n",
    "P3 = [A3, A4]\n",
    "P4 = [B3, B4]\n",
    "plt.plot(P3, P4, color = 'green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6f25c-f84f-4686-b017-14b18df27c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the thetas, theta0 is called the intercept, and theta1 is called the coefficient:\n",
    "int(reg.intercept_), int(reg.coef_)\n",
    "# 1. What coefficient did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fba28-259b-420e-9371-227032cfc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last, if you want to run a prediction, you can use the same 'predict' command, for example suppose a new Por value:\n",
    "New_Por = 8\n",
    "#let's predict the presure for that Por:\n",
    "reg.predict([[New_Por]])\n",
    "# 2. What predicted value do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc22da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good part of ML is trying to understand the data. Por is related to Prod, but it is also related to another parameter.\n",
    "# Spend some time graphing Por as x against the other columns (as y), you will find one of them that also\n",
    "# displays a linear relationship with Por. Besides Por (molecular porosity) and Prod (production output)\n",
    "# that you already know, the set includes Perm (permeability, how well water can mix with the oil), AI\n",
    "# (accoustic impedance, how well sound traverses the product), Brittle (brittleness of hard particles),\n",
    "# TOC (total organic carbon), and VR (reflectance).\n",
    "# 3. Which other linear relationship did you find to Por? Graph it and find the coefficient and the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44847f18-0ed5-4844-84e8-83f78e37fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try multivariate, also factoring TOC.\n",
    "plt.figure(figsize=(18,12))\n",
    "# A fist step is to look at if POC is also linear with Prod\n",
    "plt.plot(df[['TOC']], df[['Prod']], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00beb2b3-62d3-4fe7-b08c-d59091cee5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is linear, we can thus set our model with 2 variables\n",
    "reg.fit(df[['Por', 'TOC']],df[['Prod']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771d9bb-30d7-48a9-9e91-940135a2087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at our coefficients and intercept:\n",
    "int(reg.intercept_), reg.coef_\n",
    "# 4. Which coefficient, intercept did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78e924-f7c9-47cf-934a-5574a3c7b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax for 3-D projection\n",
    "fig1 = plt.figure(figsize=(18,12))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.scatter(df[['Por']], df[['TOC']], df[['Prod']])\n",
    "ax.set_title('Optimal pressure based on Por and TOC')\n",
    "ax.set_xlabel(\"Viscosity (Por)\")\n",
    "ax.set_ylabel(\"Granularity (TOC)\")\n",
    "ax.set_zlabel(\"Pressure (Prod)\")\n",
    "A5 = 5.5\n",
    "B5 = -0.2\n",
    "C5 = int(reg.predict([[A5, B5]]))\n",
    "A6 = 24.5 \n",
    "B6 = 2.2\n",
    "C6 = int(reg.predict([[A6, B6]]))\n",
    "P5 = [A5, A6]\n",
    "P6 = [B5, B6]\n",
    "P7 = [C5, C6]\n",
    "ax.plot(P5, P6, P7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41835f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Reproduce the same process with the other value that you found correlated to Por. Graph the relationship\n",
    "# What coefficients and intercept did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74886280-c5d0-49bd-9e3b-e531f4619bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, you may want to export your model\n",
    "import joblib\n",
    "# Save your model to a file - you should see that file in your working directory\n",
    "joblib.dump(reg, 'my_cool_joblib_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba648bd-a134-46c2-b587-7e47accfb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the model file? If it is 20Kb or less, it should fit onto an ESP 32.\n",
    "# On your target IoT device running python, you can put that model file, then then load the model. I give it a different name than 'reg'\n",
    "# so you can see how to call it:\n",
    "mj = joblib.load('my_cool_joblib_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0afd8f-979b-4545-b0e6-ba5a44fe8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then you can use that model to predict values just like before:\n",
    "mj.predict(([[5.5, -0.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61426220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also attempt to run logistic regression on this data. First, let's look at our data:\n",
    "plt.figure(figsize=(18,12))\n",
    "plt.plot(df[['Brittle']], df[['Reuse']], 'o')\n",
    "plt.title(\"Pumps issues based on grains brittleness\")\n",
    "plt.xlabel(\"Brittleness\")\n",
    "plt.ylabel(\"1 if pump could be reused, 0 if it was clogged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930f8e9-71e7-48b7-90f2-41c38fa32b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear or logistic regression, it is good practice to split the data between training and test set. \n",
    "# Let's first do that, keeping 20% for testing:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[['Brittle']], df.Reuse, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38710780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do logistic regression. We use directly the sklearn function:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we apply the function on our data, with the fit call:\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7640ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at our theta coefficients:\n",
    " model.intercept_, model.coef_\n",
    "# What coefficient and intercept did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then look at our test set, and run prediction on it:\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then you can predict which entries in the test set will be prediected for 1 and for 0.\n",
    "# 6. How many are prediected for 1, and how many for 0?\n",
    " model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the probability values, instead of merely looking at the prediction (yes/no) result.\n",
    "# The ouput displays the probability for 0 in the first column, and the probability for 1 in the second :\n",
    "model.predict_proba(X_test)\n",
    "# Is the output coherent with the prediction made above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to illustrate how the prediction works by projecting the probability onto a curve, we can generate\n",
    "# brittleness values (from 10 to 85, by jumps of 0.5), then plot the prediction:\n",
    "brittleness = np.arange(10, 85, 0.5)\n",
    "probabilities= []\n",
    "for i in brittleness:\n",
    "    p_clogs = model.predict_proba([[i]])\n",
    "    probabilities.append(p_clogs[:,1])\n",
    "plt.scatter(brittleness,probabilities)\n",
    "plt.title(\"Logistic Regression Model\")\n",
    "plt.xlabel('Brittleness')\n",
    "plt.ylabel('Status (0: clogged, 1: reused)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc045a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd692245",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec128e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2878faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn. decomposition import PCA\n",
    "pca_30 = PCA (n_components=30, random_state=2020)\n",
    "pca_30.fit (X_scaled)\n",
    "X_pca_30 = pca_30.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Variance explained by all 30 principal components =\",\n",
    "sum (pca_30.explained_variance_ratio_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43721260",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_30.explained_variance_ratio_ * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c78c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca_30.explained_variance_ratio_ ))\n",
    "plt.xlabel ('Number of components')\n",
    "plt.ylabel ('Explained variance')\n",
    "plt. savefig('elbow_plot.png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29defc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance explained by the First principal component =\",\n",
    "np. cumsum(pca_30.explained_variance_ratio_ * 100)[0])\n",
    "print (\"Variance explained by the First 2 principal components =\",\n",
    "np. cumsum (pca_30.explained_variance_ratio_ * 100)[1])\n",
    "print (\"Variance explained by the First 3 principal components =\",\n",
    "np.cumsum(pca_30.explained_variance_ratio_* 100) [2])\n",
    "print (\"Variance explained by the First 10 principal components =\",\n",
    "np. cumsum (pca_30.explained_variance_ratio_ * 100) [9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2, random_state=2020)\n",
    "pca_2.fit(X_scaled)\n",
    "X_pca_2 = pca_2.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a72883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_95 = PCA (n_components=0.95, random_state=2020)\n",
    "pca_95.fit(X_scaled)\n",
    "X_pca_95 = pca_95.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame (X_pca_95, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "df_new['label'] = cancer.target\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d3edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
